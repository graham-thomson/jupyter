{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+\n",
      "|                 PEL|            features|   probability|\n",
      "+--------------------+--------------------+--------------+\n",
      "|loA7jZOEIdPQuovau...|[-0.0941031108144...|0.472279920779|\n",
      "|9RUjG7kHNfyuf+ccL...|[-0.2426578816902...|0.547101393255|\n",
      "|9RUjG7kHNfyuf+ccL...|[-0.1134347918012...| 0.41549351372|\n",
      "|9RUjG7kHNfyuf+ccL...|[1.6445755782986,...|0.377368624576|\n",
      "|loA7jZOEIdPQuovau...|[0.06168342750406...|0.417438637431|\n",
      "+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PEL: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- probability: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df = spark.read.parquet(\"/mnt/decile_df.parquet\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.34745725989341736, 0.4102127254009247), (0.4102127254009247, 0.42469552159309387), (0.42469552159309387, 0.435580313205719), (0.435580313205719, 0.44801414012908936), (0.44801414012908936, 0.46979403495788574), (0.46979403495788574, 0.5170090794563293), (0.5170090794563293, 0.5499310493469238), (0.5499310493469238, 0.5757752060890198), (0.5757752060890198, 0.6035268306732178), (0.6035268306732178, 0.7928127646446228)]\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"probability\", df[\"probability\"].cast(FloatType()))\n",
    "\n",
    "deciles = df.approxQuantile(\"probability\", [n/10.0 for n in range(11)], 0)\n",
    "\n",
    "buckets = [(deciles[i],deciles[i+1]) for i in range(len(deciles)-1)]\n",
    "\n",
    "print buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+------+\n",
      "|                 PEL|            features|probability|decile|\n",
      "+--------------------+--------------------+-----------+------+\n",
      "|loA7jZOEIdPQuovau...|[-0.0941031108144...|  0.4722799|     5|\n",
      "|9RUjG7kHNfyuf+ccL...|[-0.2426578816902...|  0.5471014|     6|\n",
      "|9RUjG7kHNfyuf+ccL...|[-0.1134347918012...| 0.41549352|     1|\n",
      "|9RUjG7kHNfyuf+ccL...|[1.6445755782986,...| 0.37736863|     0|\n",
      "|loA7jZOEIdPQuovau...|[0.06168342750406...| 0.41743863|     1|\n",
      "+--------------------+--------------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def bucket(n):\n",
    "    for bucket_ in buckets:\n",
    "        if n >= bucket_[0] and n <= bucket_[1]:\n",
    "            return buckets.index(bucket_)\n",
    "\n",
    "bucket = udf(bucket)\n",
    "\n",
    "df_decile = df.withColumn(\"decile\", bucket(df.probability))\n",
    "df_decile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def vectorAggregator(a,b):\n",
    "    output = []\n",
    "    counter = a[0] + b[0]\n",
    "    for i in range(len(a[1])):\n",
    "        output.append(b[1][i] + a[1][i])\n",
    "    return (counter, Vectors.dense(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Decile|         Feature_Avg|\n",
      "+------+--------------------+\n",
      "|     0|[0.01342153628039...|\n",
      "|     1|[-0.0138670235491...|\n",
      "|     2|[-0.0478363636400...|\n",
      "|     3|[-0.0560319640370...|\n",
      "|     4|[-0.0264511323575...|\n",
      "|     5|[-0.0198613864002...|\n",
      "|     6|[-0.0425838609558...|\n",
      "|     7|[-0.0174017489590...|\n",
      "|     8|[0.04563753454528...|\n",
      "|     9|[0.10755071676683...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg = df_decile.rdd.map(lambda x: (x.decile, (1.0, x.features))).reduceByKey(lambda a,b: vectorAggregator(a,b)).\\\n",
    "map(lambda x: (x[0],[float(i/x[1][0]) for i in x[1][1]])).toDF(['Decile', \"Feature_Avg\"])\n",
    "\n",
    "df_agg.orderBy(\"Decile\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg.write.parquet(\"/mnt/rentrak_decile_avg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark_dfreport.builder import DataFrameReport\n",
    "\n",
    "report = DataFrameReport(\"/mnt/rentrak_decile_avg.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report.save_df(df_agg, name=\"Rentrak Decile Average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def average_by_ntile(dataframe, ntile_col, ntile=10.0):\n",
    "\n",
    "    ntile_col_index = dataframe.columns.index(ntile_col)\n",
    "    ntile_col_type = dataframe.schema[ntile_col_index].dataType\n",
    "\n",
    "    if ntile_col_type != FloatType():\n",
    "        dataframe = dataframe.withColumn(ntile_col, dataframe[ntile_col].cast(FloatType()))\n",
    "\n",
    "    ntiles = dataframe.approxQuantile(ntile_col, [n / 10.0 for n in range(11)], 0)\n",
    "\n",
    "    ntile_buckets = [(ntiles[i],ntiles[i+1]) for i in range(len(ntiles)-1)]\n",
    "\n",
    "    def bucket_probability(n):\n",
    "        for bucket_ in ntile_buckets:\n",
    "            if n >= bucket_[0] and n <= bucket_[1]:\n",
    "                return ntile_buckets.index(bucket_)\n",
    "\n",
    "    bucket_probability = udf(bucket_probability)\n",
    "\n",
    "    dataframe_w_decile = dataframe.withColumn(\"decile\", bucket_probability(dataframe[ntile_col]))\n",
    "\n",
    "    def vectorAggregator(a, b):\n",
    "        output = []\n",
    "        counter = a[0] + b[0]\n",
    "        for i in range(len(a[1])):\n",
    "            output.append(b[1][i] + a[1][i])\n",
    "        return (counter, Vectors.dense(output))\n",
    "\n",
    "    df_agg = dataframe_w_decile.rdd.map(lambda x: (x.decile, (1.0, x.features))).reduceByKey(\n",
    "        lambda a, b: vectorAggregator(a, b)). \\\n",
    "        map(lambda x: (x[0], [float(i / x[1][0]) for i in x[1][1]])).toDF(['Decile', \"Feature_Avg\"])\n",
    "\n",
    "    return df_agg.orderBy(\"Decile\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|Decile|         Feature_Avg|         Feature_Var|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|[0.01342153628039...|[0.55367137940323...|\n",
      "|     1|[-0.0138670235491...|[0.48581125350170...|\n",
      "|     2|[-0.0478363636400...|[0.37637168366152...|\n",
      "|     3|[-0.0560319640370...|[0.41283488548111...|\n",
      "|     4|[-0.0264511323575...|[0.59463570423574...|\n",
      "|     5|[-0.0198613864002...|[0.46091158972398...|\n",
      "|     6|[-0.0425838609558...|[0.37158973389797...|\n",
      "|     7|[-0.0174017489590...|[0.52197145310078...|\n",
      "|     8|[0.04563753454528...|[0.73712164018514...|\n",
      "|     9|[0.10755071676683...|[0.83036558014080...|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'andwest'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vector_std(dense_vectors):\n",
    "    \"\"\"\n",
    "\n",
    "    :param dense_vectors:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    n_columns = len(dense_vectors[0])\n",
    "\n",
    "    c_stds = []\n",
    "    c_avgs = []\n",
    "\n",
    "    for i in xrange(int(n_columns)):\n",
    "\n",
    "        c_vec = [v[i] for v in dense_vectors]\n",
    "        c_std = np.var(c_vec)\n",
    "        c_avg = np.mean(c_vec)\n",
    "        c_stds.append(c_std)\n",
    "        c_avgs.append(c_avg)\n",
    "\n",
    "    return [Vectors.dense(c_avgs), Vectors.dense(c_stds)]\n",
    "\n",
    "\n",
    "df_agg = df_decile.rdd.map(lambda x: (x.decile, [x.features]))\\\n",
    "    .reduceByKey(lambda a, b: a+b) \\\n",
    "    .map(lambda x: tuple([x[0]] + vector_std(x[1])))\\\n",
    "    .toDF(['Decile', \"Feature_Avg\", \"Feature_Var\"])\n",
    "    \n",
    "df_agg.orderBy(\"Decile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark_dfreport.builder import DataFrameReport\n",
    "\n",
    "report_w_var = DataFrameReport(\"/home/ubuntu/rentrak_dec_avg_var.xlsx\")\n",
    "\n",
    "report_w_var.save_df(df_agg, name=\"rentrak_dec_avg_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: file:/mnt/census/decile_df.parquet;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-01bc9b82e429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcensus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/mnt/census/decile_df.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcensus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/Applications/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, *paths)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \"\"\"\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/Applications/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/Applications/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u'Path does not exist: file:/mnt/census/decile_df.parquet;'"
     ]
    }
   ],
   "source": [
    "census = spark.read.parquet(\"/mnt/census/decile_df.parquet\")\n",
    "\n",
    "census.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
